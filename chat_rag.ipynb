{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamfletcher/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/haystack/core/errors.py:34: DeprecationWarning: PipelineMaxLoops is deprecated and will be remove in version '2.7.0'; use PipelineMaxComponentRuns instead.\n",
      "  warnings.warn(\n",
      "/Users/adamfletcher/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv # This is for later use but let's not forget it\n",
    "\n",
    "load_dotenv()  # load environment variables from .env file\n",
    "\n",
    "from haystack import Document\n",
    "from haystack.components.retrievers import InMemoryBM25Retriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "documents = [\n",
    "    Document(content=\"Mark lives in Berlin\"),\n",
    "    Document(content=\"Jean lives in Paris\"),\n",
    "    Document(content=\"Mark speaks Turkish\"),\n",
    "    Document(content=\"Jean was born in Belgium\"),\n",
    "    ]\n",
    "\n",
    "# Add documents to the document_store\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents(documents)\n",
    "\n",
    "# Create the retriever with the document_store\n",
    "retriever = InMemoryBM25Retriever(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x3175cbc80>\n",
       "🚅 Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - generator: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> generator.prompt (str)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack import Pipeline\n",
    "\n",
    "# Create prompt builder component\n",
    "prompt_template = \"\"\"\n",
    "Given these documents, answer the question.\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "\n",
    "# Create generator component\n",
    "model_name = \"gpt-4o-mini\"\n",
    "llm = OpenAIGenerator(model=model_name)\n",
    "\n",
    "# Build pipeline and create connections between components\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"generator\", llm)\n",
    "\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"generator.prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "rag_pipeline.draw(Path(\"pipeline.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Where does Mark live?\n",
      "Assistant: Mark lives in Berlin.\n",
      "\n",
      "User: What language does Mark speak?\n",
      "Assistant: Mark speaks Turkish.\n",
      "\n",
      "User: Where was Jean born?\n",
      "Assistant: Jean was born in Belgium.\n",
      "\n",
      "User: Where does Jean live?\n",
      "Assistant: Jean lives in Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "questions = [\n",
    "    \"Where does Mark live?\",\n",
    "    \"What language does Mark speak?\",\n",
    "    \"Where was Jean born?\",\n",
    "    \"Where does Jean live?\",\n",
    "]\n",
    "\n",
    "for query in questions:\n",
    "    result = rag_pipeline.run({\"retriever\": {\"query\": query}, \"prompt_builder\": {\"question\": query}})\n",
    "    llm_response = result[\"generator\"][\"replies\"][0]\n",
    "\n",
    "    print(f\"User: {query}\")\n",
    "    print(f\"Assistant: {llm_response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new, complete chatbot pipeline with memory\n",
    "# We need to create each component again in order to be able to use them in a new pipeline\n",
    "\n",
    "# Create the retriever with the document_store\n",
    "retriever = InMemoryBM25Retriever(document_store)\n",
    "\n",
    "# Create prompt builder component\n",
    "prompt_template = \"\"\"\n",
    "Given these documents and the current conversation, answer the question.\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "Question: {{question}}\n",
    "\n",
    "Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "Current Conversation:\n",
    "{% for chat in chat_history %}\n",
    "    {{ chat.role.name }}:  {{ chat.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_builder = PromptBuilder(template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x317641520>\n",
       "🚅 Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - generator: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> generator.prompt (str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create generator component\n",
    "model_name = \"gpt-4o-mini\"\n",
    "llm = OpenAIGenerator(model=model_name)\n",
    "\n",
    "# Build pipeline and create connections between components\n",
    "chat_pipeline = Pipeline()\n",
    "chat_pipeline.add_component(\"retriever\", retriever)\n",
    "chat_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "chat_pipeline.add_component(\"generator\", llm)\n",
    "\n",
    "chat_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "chat_pipeline.connect(\"prompt_builder.prompt\", \"generator.prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat_history that will store ChatMessages.\n",
    "from haystack.dataclasses.chat_message import ChatMessage\n",
    "from typing import List\n",
    "chat_history: List[ChatMessage] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_response: Mark lives in Berlin.\n",
      "llm_response: You asked me earlier, \"Where does Mark live?\" and my response was \"Mark lives in Berlin.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Where does Mark live?\"\n",
    "result = chat_pipeline.run({\"retriever\": {\"query\": query}, \n",
    "                            \"prompt_builder\": {\"question\": query, \n",
    "                                               \"chat_history\": chat_history}\n",
    "                                               })\n",
    "llm_response = result[\"generator\"][\"replies\"][0]\n",
    "print(f\"llm_response: {llm_response}\")\n",
    "chat_history.append(ChatMessage.from_user(query))\n",
    "chat_history.append(ChatMessage.from_assistant(llm_response))\n",
    "\n",
    "query = \"What did I ask you earlier and what was your response?\"\n",
    "result = chat_pipeline.run({\"retriever\": {\"query\": query}, \"prompt_builder\": {\"question\": query, \"chat_history\": chat_history}})\n",
    "llm_response = result[\"generator\"][\"replies\"][0]\n",
    "print(f\"llm_response: {llm_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n"
     ]
    }
   ],
   "source": [
    "print('k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
