{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamfletcher/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/haystack/core/errors.py:34: DeprecationWarning: PipelineMaxLoops is deprecated and will be remove in version '2.7.0'; use PipelineMaxComponentRuns instead.\n",
      "  warnings.warn(\n",
      "/Users/adamfletcher/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(id=62fad790ad2af927af9432c87330ed2ea5e31332cdec8e9d6235a5105ab0aaf5, content: 'My name is Wolfgang and I live in Berlin', score: 0.8843885516161921)\n"
     ]
    }
   ],
   "source": [
    "# from haystack import Document\n",
    "# from haystack import Pipeline\n",
    "# from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "# from haystack.components.embedders import OpenAITextEmbedder, OpenAIDocumentEmbedder\n",
    "# from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "# document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "# documents = [Document(content=\"My name is Wolfgang and I live in Berlin\"),\n",
    "#              Document(content=\"I saw a black horse running\"),\n",
    "#              Document(content=\"Germany has many big cities\")]\n",
    "\n",
    "# document_embedder = OpenAIDocumentEmbedder()\n",
    "# documents_with_embeddings = document_embedder.run(documents)['documents']\n",
    "# document_store.write_documents(documents_with_embeddings)\n",
    "\n",
    "# query_pipeline = Pipeline()\n",
    "# query_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\n",
    "# query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "\n",
    "\n",
    "# query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "\n",
    "# query = \"Who lives in Berlin?\"\n",
    "\n",
    "# result = query_pipeline.run({\"text_embedder\":{\"text\": query}})\n",
    "\n",
    "# print(result['retriever']['documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Add the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamfletcher/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/haystack/core/errors.py:34: DeprecationWarning: PipelineMaxLoops is deprecated and will be remove in version '2.7.0'; use PipelineMaxComponentRuns instead.\n",
      "  warnings.warn(\n",
      "/Users/adamfletcher/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x31ce01070>\n",
       "🚅 Components\n",
       "  - text_embedder: OpenAITextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.embedders import OpenAITextEmbedder, OpenAIDocumentEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "generator = OpenAIGenerator(model=\"gpt-4o-mini\")\n",
    "\n",
    "documents = [Document(content=\"My name is Wolfgang and I live in Berlin\"),\n",
    "             Document(content=\"I saw a black horse running\"),\n",
    "             Document(content=\"Germany has many big cities\")]\n",
    "\n",
    "document_embedder = OpenAIDocumentEmbedder()\n",
    "documents_with_embeddings = document_embedder.run(documents)['documents']\n",
    "document_store.write_documents(documents_with_embeddings)\n",
    "\n",
    "query_pipeline = Pipeline()\n",
    "query_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\n",
    "query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "query_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "query_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "query_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "query_pipeline.connect(\"prompt_builder\", \"llm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolfgang lives in Berlin.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Who lives in Berlin?\"\n",
    "\n",
    "result = query_pipeline.run({\"text_embedder\":{\"text\": query}, \"prompt_builder\": {\"question\": query}})\n",
    "\n",
    "# print(result['retriever']['documents'][0])\n",
    "print(result['llm']['replies'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can I make a chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n"
     ]
    },
    {
     "ename": "PipelineConnectError",
     "evalue": "Cannot connect 'retriever' with 'llm': no matching connections available.\n'retriever':\n - documents: List[Document]\n'llm':\n - messages: List[ChatMessage] (available)\n - streaming_callback: Optional[Callable[]] (available)\n - generation_kwargs: Optional[Dict[str, Any]] (available)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineConnectError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m query_pipeline\u001b[38;5;241m.\u001b[39madd_component(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m, generator)\n\u001b[1;32m     25\u001b[0m query_pipeline\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embedder.embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever.query_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mquery_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretriever\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/haystack/core/pipeline/base.py:526\u001b[0m, in \u001b[0;36mPipelineBase.connect\u001b[0;34m(self, sender, receiver)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    523\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot connect \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msender_component_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreceiver_component_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno matching connections available.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         )\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineConnectError(msg)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(possible_connections) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# There's only one possible connection, use it\u001b[39;00m\n\u001b[1;32m    530\u001b[0m     sender_socket \u001b[38;5;241m=\u001b[39m possible_connections[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mPipelineConnectError\u001b[0m: Cannot connect 'retriever' with 'llm': no matching connections available.\n'retriever':\n - documents: List[Document]\n'llm':\n - messages: List[ChatMessage] (available)\n - streaming_callback: Optional[Callable[]] (available)\n - generation_kwargs: Optional[Dict[str, Any]] (available)"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.embedders import OpenAITextEmbedder, OpenAIDocumentEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
    "\n",
    "documents = [Document(content=\"My name is Wolfgang and I live in Berlin\"),\n",
    "             Document(content=\"I saw a black horse running\"),\n",
    "             Document(content=\"Germany has many big cities\")]\n",
    "\n",
    "document_embedder = OpenAIDocumentEmbedder()\n",
    "documents_with_embeddings = document_embedder.run(documents)['documents']\n",
    "document_store.write_documents(documents_with_embeddings)\n",
    "\n",
    "query_pipeline = Pipeline()\n",
    "query_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\n",
    "query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "# query_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "query_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "query_pipeline.connect(\"retriever\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineConnectError",
     "evalue": "Cannot connect 'prompt_builder' with 'llm': no matching connections available.\n'prompt_builder':\n - prompt: str\n'llm':\n - messages: List[ChatMessage] (available)\n - streaming_callback: Optional[Callable[]] (available)\n - generation_kwargs: Optional[Dict[str, Any]] (available)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineConnectError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m rag_pipe\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedder.embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever.query_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m rag_pipe\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_builder.documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mrag_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_builder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/find-techspike-rag/.venv/lib/python3.12/site-packages/haystack/core/pipeline/base.py:526\u001b[0m, in \u001b[0;36mPipelineBase.connect\u001b[0;34m(self, sender, receiver)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    523\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot connect \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msender_component_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreceiver_component_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno matching connections available.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         )\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineConnectError(msg)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(possible_connections) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# There's only one possible connection, use it\u001b[39;00m\n\u001b[1;32m    530\u001b[0m     sender_socket \u001b[38;5;241m=\u001b[39m possible_connections[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mPipelineConnectError\u001b[0m: Cannot connect 'prompt_builder' with 'llm': no matching connections available.\n'prompt_builder':\n - prompt: str\n'llm':\n - messages: List[ChatMessage] (available)\n - streaming_callback: Optional[Callable[]] (available)\n - generation_kwargs: Optional[Dict[str, Any]] (available)"
     ]
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "rag_pipe = Pipeline()\n",
    "rag_pipe.add_component(\"embedder\", OpenAITextEmbedder())\n",
    "rag_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "rag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "rag_pipe.add_component(\"llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\"))\n",
    "\n",
    "rag_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipe.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-ada-002',\n",
       "   'usage': {'prompt_tokens': 5, 'total_tokens': 5}}},\n",
       " 'llm': {'replies': ['The context does not provide any information about a person named Mark, so it is not possible to answer where he lives. The only person mentioned is Wolfgang, who lives in Berlin.'],\n",
       "  'meta': [{'model': 'gpt-4o-mini-2024-07-18',\n",
       "    'index': 0,\n",
       "    'finish_reason': 'stop',\n",
       "    'usage': {'completion_tokens': 36,\n",
       "     'prompt_tokens': 54,\n",
       "     'total_tokens': 90,\n",
       "     'completion_tokens_details': CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0),\n",
       "     'prompt_tokens_details': PromptTokensDetails(audio_tokens=0, cached_tokens=0)}}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Where does Mark live?\"\n",
    "rag_pipe.run({\"embedder\": {\"text\": query}, \"prompt_builder\": {\"question\": query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_func(query: str):\n",
    "    result = rag_pipe.run({\"embedder\": {\"text\": query}, \"prompt_builder\": {\"question\": query}})\n",
    "\n",
    "    return {\"reply\": result[\"llm\"][\"replies\"][0]}\n",
    "\n",
    "WEATHER_INFO = {\n",
    "    \"Berlin\": {\"weather\": \"mostly sunny\", \"temperature\": 7, \"unit\": \"celsius\"},\n",
    "    \"Paris\": {\"weather\": \"mostly cloudy\", \"temperature\": 8, \"unit\": \"celsius\"},\n",
    "    \"Rome\": {\"weather\": \"sunny\", \"temperature\": 14, \"unit\": \"celsius\"},\n",
    "    \"Madrid\": {\"weather\": \"sunny\", \"temperature\": 10, \"unit\": \"celsius\"},\n",
    "    \"London\": {\"weather\": \"cloudy\", \"temperature\": 9, \"unit\": \"celsius\"},\n",
    "}\n",
    "\n",
    "\n",
    "def get_current_weather(location: str):\n",
    "    if location in WEATHER_INFO:\n",
    "        return WEATHER_INFO[location]\n",
    "\n",
    "    # fallback data\n",
    "    else:\n",
    "        return {\"weather\": \"sunny\", \"temperature\": 21.8, \"unit\": \"fahrenheit\"}\n",
    "\n",
    "available_functions = {\"rag_pipeline_func\": rag_pipeline_func, \"get_current_weather\": get_current_weather}\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
    "response = None\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "def chatbot_with_fc(message, history):\n",
    "    messages.append(ChatMessage.from_user(message))\n",
    "    response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "    while True:\n",
    "        # if OpenAI response is a tool call\n",
    "        if response and response[\"replies\"][0].meta[\"finish_reason\"] == \"tool_calls\":\n",
    "            function_calls = json.loads(response[\"replies\"][0].content)\n",
    "            print(response[\"replies\"][0])\n",
    "            for function_call in function_calls:\n",
    "                ## Parse function calling information\n",
    "                function_name = function_call[\"function\"][\"name\"]\n",
    "                function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "\n",
    "                ## Find the correspoding function and call it with the given arguments\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_response = function_to_call(**function_args)\n",
    "\n",
    "                ## Append function response to the messages list using `ChatMessage.from_function`\n",
    "                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n",
    "                response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "        # Regular Conversation\n",
    "        else:\n",
    "            messages.append(response[\"replies\"][0])\n",
    "            break\n",
    "    return response[\"replies\"][0].content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
